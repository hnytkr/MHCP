{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MHP\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "import os\n",
    "from huggingface_hub import login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"hugging_face_access_token\")\n",
    "\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Hugging_face_access_token\"\n",
    "\n",
    "chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_paths):\n",
    "    global chain\n",
    "    if not file_paths:\n",
    "        return \"No files uploaded. Please upload PDF files.\"\n",
    "    \n",
    "    if not all(file_path.endswith('.pdf') for file_path in file_paths):\n",
    "        return \"Please upload valid PDF files.\"\n",
    "    \n",
    "    try:\n",
    "        # Load and process the PDFs\n",
    "        documents = []\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                print(f\"Processing file: {file_path}\")  # Debugging\n",
    "                loader = PyMuPDFLoader(file_path)\n",
    "                loaded_docs = loader.load()\n",
    "                documents.extend(loaded_docs)\n",
    "                print(f\"Loaded {len(loaded_docs)} documents from {file_path}\")  # Debugging\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")  # Debugging\n",
    "                return f\"Error loading {file_path}: {str(e)}\"\n",
    "        \n",
    "        print(f\"Total documents loaded: {len(documents)}\")  # Debugging\n",
    "        \n",
    "        if not documents:\n",
    "            return \"No text found in the uploaded PDFs.\"\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        print(\"Embeddings created successfully.\")  # Debugging\n",
    "        \n",
    "        # Split text into chunks\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        split_text = text_splitter.split_documents(documents)\n",
    "        print(f\"Text after splitting: {split_text[:2]}\")  # Debugging\n",
    "        \n",
    "        if not split_text:\n",
    "            return \"No text found after splitting the documents.\"\n",
    "        \n",
    "        # Create vector store\n",
    "        db = Chroma.from_documents(split_text, embeddings)\n",
    "        print(f\"Chroma database contains {len(db)} vectors.\")  # Debugging\n",
    "        \n",
    "        # Initialize LLM\n",
    "        llm = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\")  # Alternative LLM for QA\n",
    "        print(\"LLM initialized successfully.\")  # Debugging\n",
    "        \n",
    "        # Create RetrievalQA chain\n",
    "        chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "        print(\"QA chain initialized successfully.\")  # Debugging\n",
    "        \n",
    "        return \"Dataset processed successfully! You can now ask questions.\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDFs: {e}\")  # Debugging\n",
    "        return f\"Error processing PDFs: {str(e)}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def answer_query(query):\n",
    "    if chain is None:\n",
    "        return \"Please upload and process Dataset(Pdf's) first.\"\n",
    "    \n",
    "    try:\n",
    "        result = chain.run(query)\n",
    "        \n",
    "        # Extract only the relevant answer and format it properly\n",
    "        if \"{\" in result:  # Check if response contains unwanted JSON-like text\n",
    "            start_idx = result.find(\"Helpful Answer:\")\n",
    "            if start_idx != -1:\n",
    "                result = result[start_idx:]  # Keep only the answer part\n",
    "        \n",
    "        formatted_result = f\"{query}\\n {result.strip()}\"\n",
    "        \n",
    "        print(f\"Query result: {formatted_result}\")  # Debugging\n",
    "        return formatted_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")  # Debugging\n",
    "        return f\"Error during query: {str(e)}\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to programmatically upload and process PDFs\n",
    "def programmatic_upload_and_process():\n",
    "    # List of PDF file paths to upload programmatically\n",
    "    pdf_paths = [\n",
    "        \"combinedpdf_dataset.pdf\", \n",
    "        \"Bhagavad_Gita_As_It_Is.pdf\",\n",
    "        # \"path/to/your/file3.pdf\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Call the process_pdf function directly with the file paths\n",
    "        status = process_pdf(pdf_paths)\n",
    "        \n",
    "        # Ensure the chain is initialized\n",
    "        global chain\n",
    "        if chain is None:\n",
    "            raise Exception(\"Failed to initialize the QA chain.\")\n",
    "            \n",
    "        return \"Dataset processed successfully! You can now ask questions.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during programmatic upload: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MHP\\myenv\\Lib\\site-packages\\gradio\\components\\chatbot.py:282: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio Interface (Assistant Bot Style)\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    # Title and description\n",
    "    gr.Markdown(\"# ðŸ¤– Mental Health Counselling Bot\")\n",
    "    gr.Markdown(\"Welcome to the Mental Health Counselling Assistant Bot! I can help you answer questions about your Mental Health.\")\n",
    "    \n",
    "    # Chatbot interface\n",
    "    chatbot = gr.Chatbot(label=\"Chat with the Assistant\")\n",
    "    msg = gr.Textbox(label=\"Ask a question\", placeholder=\"Type your question here...\")\n",
    "    clear = gr.Button(\"Clear Chat\")\n",
    "    \n",
    "    # Function to handle chat interactions\n",
    "    def respond(message, chat_history):\n",
    "        if chain is None:\n",
    "            chat_history.append((message, \"Dataset's are still being processed. Please wait...\"))\n",
    "            return chat_history\n",
    "        response = answer_query(message)\n",
    "        chat_history.append((message, response))\n",
    "        return chat_history\n",
    "    \n",
    "    # Function to clear the chat\n",
    "    def clear_chat():\n",
    "        return []\n",
    "    \n",
    "    # Link Gradio events to functions\n",
    "    msg.submit(respond, [msg, chatbot], chatbot)\n",
    "    clear.click(clear_chat, None, chatbot, queue=False)\n",
    "    \n",
    "    # Automatically process PDFs when the interface loads\n",
    "    def on_load():\n",
    "        status = programmatic_upload_and_process()\n",
    "        return [(None, status)]  # Display status in chat\n",
    "    \n",
    "    demo.load(on_load, None, chatbot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
